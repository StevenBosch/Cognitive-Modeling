\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{apacite}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{listings}

\title{Memory Driven Temporal Preparation: A Cognitive Model}
 
\author{{\large \bf Steven Bosch (s1861948 - s.bosch.8@student.rug.nl)} \\
  Faculty of Mathematics and Natural Sciences}


\begin{document}
\maketitle

\begin{abstract}
There are multiple theories on how human cognition is able to time cue target intervals. One such theories encompasses the \textit{hazard function}, which describes that the conditional probability of the occurrence of a target event after the presentation of a cue increases over time, given that the event has not yet occurred. On the other hand, Los et al. (2014) developed the \textit{multiple trace theory of temporal preparation} (MTP) that states that every new trial causes a memory trace to be created, which influences the next trial. In an experiment they showed that this seems to be the case indeed. This study discusses a cognitive model that incorporates both the hazard function and the MTP. In comparison with the experiment results the model performed quite well, but it falls short in some respects.

\textbf{Keywords:} 
temporal preparation; cue target interval timing; Hazard function; long term memory
\end{abstract}

\section{Introduction}
The human brain has many traits that we are hardly aware of during our everyday lives, even though these traits have a great impact on our experiences and actions. One such trait is the ability of cue target interval timing. Both consciously and unconsciously this ability is exercised in numerous situations, such as waiting for traffic lights, expecting a sound when something falls, or simply expecting the second hand of a clock to move to the next second. 

One theory that tries to explain this cognitive ability relies on the \textit{hazard function}. This function states that the conditional probability of the occurrence of a target event after the presentation of a cue increases over time, given that the event has not yet occurred. While the hazard function has been successfully used to predict interval timing behaviour \cite{Nobre, Vangkilde}, it does not provide an explanation as to the cognitive processes behind it. Moreover it does not take into account memory traces of preceding timing experiences. Over the past years research began on just this basis \cite{Los1, Howard, Taatgen}. Los et al. (2014) developed what they call the \textit{multiple trace theory of temporal preparation} (MTP), in which every new trial causes a memory trace to be created, storing a temporal profile of that trial. This memory trace subsequently contributes to the preparation of subsequent trials. 

A recent experiment by Los et al. indicates that temporal preparation is indeed driven by memory \cite{Los2}. In the experiment different groups of participants were presented with different distributions of foreperiods between temporal cues and target stimuli. Three of these experiments showed a transfer effect of this manipulation in a test phase where all participants received the same uniform distribution, indicating memory influence. 

Using a cognitive model might give us more insight into he workings of this particular cognitive process. Therefore this paper discusses a model that tries to capture (some of) the cognitive processes behind the phenomenon that the experiment by Los et al. (2015) brought to light.

\section{Experiment}
\subsection{Method}
In Los et al.'s (2015) experiment sixty-four students were randomly assigned to one of two groups, each consisting of 32 participants. The participants were asked to sit behind a computer screen resting their left index finger on the \textit{z} key and their right index finger on the \textit{m} key. Each trial started with the appearance of a black plus sign, \textit{S1}, in the middle of the screen. Subsequently after a given foreperiod of either 400, 800, 1200 or 1600 ms a second signal, \textit{S2}, appeared. Seeing the signal, the participants had to press either \textit{z}, when \textit{S2} appeared left, or \textit{m} when it appeared right. They were instructed to perform this action as fast as they could. After the response the screen turned black and after an interval of 1.5 seconds the next trial would start.

Every participant had to complete 5 blocks of 120 trials each. The distribution of trials within a block was either uniform (30 trials of each foreperiod), exponential (64, 32, 16 and 8 trials for foreperiods of 400, 800, 1200 and 1600 ms respectively) or anti-exponential (8, 16, 32 and 64 trials for foreperiods of 400, 800, 1200 and 1600 ms respectively). Table \ref{Table1} shows the foreperiod distributions of the blocks per group. For both of the groups block 1, 4 and 5 were uniform, whereas block 2 and 3 were either exponential or anti-exponential.

In the experiment the participants were informed every block about their mean response time (RT) and the percentage of correct responses of that block. Furthermore after block 3 they were informed of the distributions of the preceding and succeeding two blocks.

\begin{table}
	\centering
	\caption{Successive foreperiod distributions across blocks for group 1 and 2 (Uni $=$ uniform, Exp $=$ exponential, Anti-exp $=$ anti-exponential).}
	\begin{tabular}{c|c|c|c|c|c}
		Block & 1 & 2 & 3 & 4 & 5 \\
		\hline
		Group 1 & Uni & Exp & Exp & Uni & Uni \\
		Group2 & Uni & Anti-exp & Anti-exp & Uni & Uni
	\end{tabular}
	\label{Table1}
\end{table}

\subsection{Results}
Figure \ref{LosFigure} shows the results of Los et al.'s experiment. Los et al. applied a mixed Analysis of Variance (ANOVA) on the data of each block, with group and foreperiod as factors. They report a strong main effect of foreperiod in all blocks, $F (1, 62) \geq 58.63$ and $p < 0.001$. Furthermore they reported a significant main effect of group in block 2, $F (1, 62) = 13.81$, $p < 0.001$ and 3, $F(1, 62) = 13.95$, $p < 0.001$, but not in the other blocks. The interaction between group and foreperiod was not significant in block 1, $F(1, 62) < 1$, but it was in the other four blocks, with blocks 2 and 3 having $F (1, 62) \geq 69.64$, $p < 0.001$, and blocks 4 and 5 having $F (1, 62) \geq 13.12$ and $p \leq 0.001$.

Figure \ref{LosFigure} shows that for the first uniform distribution, when the participants do not have any prior experience with the experiment yet, their mean RT for a foreperiod of 400 ms is around 360 ms. It decreases exponentially to around 320 ms for a foreperiod of 1600. In blocks 2 and 3 we see a clear difference between the exponential and anti-exponential distribution. The mean RT for the exponential group decreases greatly for shorter foreperiods, while the opposite happens for the anti-exponential group: the mean RT increases especially for the shorter foreperiods. In the final two blocks we can still see remnants of the preceding distributions, with slight differences between the two groups of participants (as discussed the interaction between group and foreperiod was significant in these blocks as well).

As discussed in the introduction, from these results it appears that the cognitive process of cue target interval timing involves more than just the hazard function. Indeed, otherwise there would have been no difference between the mean RTs of the different blocks. The results of this experiment clearly showed that the distribution of foreperiods does affect temporal preparation. When many short stimuli have appeared in the recent experiences of a participant, he will be more prone to react faster after a short foreperiod. Similarly when many long stimuli have appeared in his recent experiences, the response time is longer for all foreperiods, but especially for the shorter ones. This observation brings us to our cognitive model.

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{Los1.png}
	\caption{Los et al.'s experiment results of mean response time as a function of group, block and foreperiod. Illustration from Los et al. (2015).}
	\label{LosFigure}
\end{figure}

\section{Model}
\subsection{Method}
The model consists of more general timing and memory functions, based upon functions that were proposed in the literature, and two components specifically designed for the timing task addressed in the experiment. 

The timing function is modelled by \citet{Taatgen1} as an Act-R module, the `temporal module'.\footnote{Act-R is a cognitive architecture, see http://act-r.psy.cmu.edu/.} This module models the idea that people internally measure an interval in ticks, which are counted with increasingly long and noisy intervals. Furthermore our model uses a declarative memory function (DM-module) that is proposed in \citet{Taatgen}, in which memories are stored as encounters of a specific chunk in the declarative memory. Subsequently, the activation of every chunk is calculated to determine how `fresh' in memory it is, according to the following function:
\begin{equation}
	A(t) = log(\sum_{enc = 0}^{encs}(t - t_{enc})^{-d}))
\end{equation}
Here $t$ is the current time, $t_{enc}$ is the time when an encounter of that specific chunk is added and $d$ is a constant.

For this specific cue target interval timing problem there were two more main components that needed to be modelled, in order for the model to produce the results generated in the experiment:
\begin{enumerate}
	\item A hazard function
	\item Memory influence on temporal preparation
\end{enumerate}

\paragraph{A hazard function}
A hazard function is necessary, because the first block in the experiment clearly showed that without memory of past cue target timing experiences, the mean response times decrease as the foreperiod becomes longer, down to what seems to be a horizontal asymptote.\footnote{Note that this study only discusses the time intervals discussed in Los et al.'s paper. Different mechanisms might come into play for shorter or longer intervals.} The Hazard function can be applied here: the probability of $S2$ appearing keeps increasing as long as it has not appeared, and with it the temporal preparation as well. 

The fact that the plots show a negative `$1/x$' type of function might be the result of a number of things. First, there is a limit to how fast a human being can respond: he has to observe the signal first, after which the image has to be processed and recognized. Finally the procedural step has to be taken to press the (right) button. Since all of these processes take time, it is logical that there is some minimum response time, that the response times approach as the participants become more prepared. Second, a participant cannot expect the experiment to last forever. This makes it logical that the first milliseconds its expectation increases (so its response time decreases) at a higher rate than at a later time. 

Finally, one last assumption is made in modelling the hazard function: next to a minimum response time, we also assume a maximum response time. No matter how ill-prepared a participant may be for the second signal, he will still respond within a certain amount of time, unless of course he is distracted in one way or another, but that response would probably be counted as an outlier.

The implementation of the Hazard function used in the model is the following:
\begin{equation}
	R = R_b + maxDelay * e^{-\frac{ticks^\alpha}{\beta}}
\end{equation}
Here $R$ is the response time, $R_b$ the minimum (base) response time, maxDelay the maximum number of milliseconds a response can be delayed, ticks the number of ticks counted during the foreperiod, and $\alpha$ and $\beta$ parameters that depend on memory (see the next paragraph). This study uses a base response of 320 ms and a maximum delay of 150 ms, meaning that all of the responses fall within 320 and 470 ms. The exponential factor in the function makes sure that a longer foreperiod is always responded to more quickly than a shorter one. It depends on $\alpha$ and $\beta$ what the magnitude of this difference is.

\paragraph{Memory influence}
From the results of the Los et al. experiment it appears that memory has a direct influence on the preparation rate for the second signal, so this had to implemented in the model as well. As we see from the results, with more short intervals in memory participants were temporally prepared more quickly and responded faster to (primarily the short) intervals. Alternatively with more long intervals in memory participants would take longer to be prepared, which not only affected the short intervals, but even the long intervals (see figure \ref{LosFigure}, the graphs of the second and third block). One explanation for this phenomenon is that a participant unconsciously determines the probabilities of a trial being a short or long interval depending on the distribution of intervals it has in its memory. Depending on this estimate the participant would (unconsciously) either prepare himself quickly for a short interval or `take it slow' because there was no immediate need for preparation. 

The participants probably did not experience a sense of exact knowledge concerning the distribution of foreperiods in the block they were in. They probably did however have a more general (unconscious) sense of the foreperiods being predominantly shorter or longer in their past experiences. In this model it is assumed the participants' cognition operated based on these possibilities for the composition of the (recent) foreperiod distribution, together with the third option of the long and short periods being represented approximately equally.

This is modelled in the following way. The prior probability of every encountered foreperiod (in `ticks') is calculated as the activation of a chunk divided by the total activation of all the chunks, based on all the encounters per chunk (a chunk represents a specific number of ticks). Since the foreperiods are converted to ticks, which is a noisy process, multiple chunks can represent the same foreperiod. Using these prior probabilities, a combined probability on all the shorter foreperiod (400 and 800 ms) encounters is calculated as:
\begin{equation}
	P(short) = \sum_{i = 0}^{m} P_i
\end{equation}
in which $i$ is the number of ticks and $m$ is the maximum number of ticks that falls under `short' interval. One would consider this to be noisy, but since the conversion from time to ticks is already a noisy process, this parameter was hard coded.

Using this probability the reaction time is now calculated using equation 2, with the following rules:
\begin{itemize}
	\item if $P(short) < T_1$: use $\alpha_1$ and $\beta_1$
	\item else if $P(short) < T_2$: use $\alpha_2$ and $\beta_2$
	\item else use $\alpha_3$ and $\beta_3$
\end{itemize}
These rules say the following: if there is a low probability on a short interval (lower than the first threshold $T_1$), use the parameters $\alpha_1$ and $\beta_1$, which are set at ``taking the time in preparing for the second signal''. If this is not the case and the probability is about half for either short or long intervals ($T_1 \leq P(short) < T_2$), use the parameters $\alpha_2$ and $\beta_2$ (``the standard situation''). Finally if that is not the case, meaning there is a higher probability on short intervals ($T_2 \leq P(short)$), use parameters $\alpha_3$ and $\beta_3$ (``prepare for the second signal quickly'').\footnote{These parameters need to be tuned to get the correct result. Our model uses: $T_1 = 0.35; T_2 = 0.55; \alpha_1 = 2.6; \beta_1=2400; \alpha_2 = 2; \beta_2 = 200; \alpha_3 = 0.5; \beta_3 = 1$.}
\bigskip

\noindent Just as in the Los et al.'s experiment the experiment testing the model was run with two groups of 32 `participants' each, with the same foreperiod distributions per group.

\subsection{Results}
\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{5blocks2.png}
	\caption{Model results of mean response time as a function of group, block and foreperiod.}
	\label{5blocks}
\end{figure}
Figure \ref{5blocks} shows the mean RT of the model as function of foreperiod, group and block. We applied an ANOVA on the data of every block, with foreperiod and group as factors. As with the experiment of Los et al. the ANOVA shows a strong main effect of foreperiod in all blocks, $F(1,64) \geq 86.38$, $p < 0.001$. Apart from the first block, all blocks show a strong main effect of group as well, $F(1,64) \geq 42.91$, $p < 0.001$. Finally apart from the first block, all blocks show a strong main effect of the interaction between group and foreperiod, $F(1,64) \geq 344.4$, $p < 0.001$.

The figure and ANOVA results shows that in the first block both of the groups yield approximately the same response times. For the latter four blocks this is not the case however, for these blocks the ANOVA yielded significant differences between the two groups. From the figure we can see that group 1, which received the exponential distributions (so mostly short intervals) responded faster to the short intervals, especially in the third and fourth block, in which there is little difference between the response times for short and long foreperiods ($\leq 10 ms$). Group 2, who received the anti-exponential distributions, responded slower in blocks 2-4, especially so when given short foreperiods, but slightly for long foreperiods as well. In the last block we see the two groups having almost the same response times again (although the difference is significant as mentioned above).

\section{Discussion}
Comparing the results, we can observe that the model approximates the results of the experiment by Los et al. well in some areas, while in others it does not.

First of all the `standard' situation, in which the distribution of both groups is uniform, seems to be approximated by the model quite well. Both in the experiment and in the model results the foreperiod of 400 ms is responded to in around 360 ms, while that of 1600 ms in around 320 ms. So the `hazardous' aspect of temporal preparation seems to have been covered adequately by the model.

Furthermore we can see that especially the third block is approximated quite precisely, although the experiment's data are more linear for the first two foreperiods than that of the model. Since most of the other curves shown in figure \ref{LosFigure} have a less linear nature, it might be that this stems from the dataset of the experiment not being large enough. Another possibility is of course that the response time is in fact more linearly dependent on the foreperiod than the model currently works with. As discussed above however, there has to be an asymptote somewhere down the line, since a human cannot respond instantly, let alone back in time. Therefore a less linear relationship seems more plausible. 

One respect in which the results do differ noteworthy is how the memory is involved. Figures \ref{LosFigure} and \ref{5blocks} show that group 1 of the model does not quite follow `the pace' of group 1 of the experiment. Whereas in the experiment block 2 shows clearly faster response times for shorter foreperiods, the model results show the same curve as in block 1. However, in block 3 and 4 the model shows the result it should have shown in block 2 and 3 (with the faster response times). In other words, it seems that the declarative memory functionality of group 1 in the model `falls behind', so either the long-term memory has too much effect, or the short-term memory does not have enough effect, or both. For group 2 we see this effect as well but to a far less extent (it is only visible in block 4, which has too high response times for short foreperiods). It seems however that this problem might be a matter of `finetuning' the declarative memory, or the parameters with which its influence on the temporal preparation is determined, such as the function parameters $\alpha$ and $\beta$ or the threshold parameters $T_1$ and $T_1$.

There is one aspect that this model cannot imitate: there is no situation in which the model produces response rates for longer foreperiods that are higher than those of shorter foreperiods. Although we hardly see this phenomenon in the experimental data (slightly in figure \ref{LosFigure}, group 1, block 3 and 4) it is a thinkable scenario that, as the time of the foreperiod progresses, attention fades and the temporal preparedness with it. This is however not a phenomenon that was part of the outset of this study. Moreover, in the limited time that is represented by the intervals in this experiment, this phenomenon is not yet a significant feature of cognitive behaviour. When longer intervals would be studied, this would however prove a valid research question.

Another drawback to the current model is that it has many parameters that have influence on the results. This makes it hard to pinpoint exactly what caused differences in output. Are the parameter settings at fault or is the whole model architecture inherently wrong?

This brings us to our conclusion. Aside from the drawbacks the model suffers from, the model approximates the experiment results quite well. However, it is hard to tell whether the actual cognitive processes are modelled in a sufficient manner. The model is based on the idea that the declarative memory gives an approximation of whether the intervals in the past seconds or minutes have been predominantly long or short. While this is an intuitive approach, this assumption needs more research to properly assert. 


\bibliographystyle{apacite}
\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}
\bibliography{bibliography}

\end{document}
